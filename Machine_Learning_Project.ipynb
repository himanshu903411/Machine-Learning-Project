{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Importing Libraries & Dataset\n",
        "\n",
        "# __init__.py\n",
        "\"\"\"\n",
        "Crypto Analysis Package\n",
        "\"\"\"\n",
        "# data_loader.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def load_data(file_path_16, file_path_17):\n",
        "    \"\"\"\n",
        "    Load datasets from the specified CSV files and combine them into one DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path_16: str, path to the CSV file for March 16th\n",
        "    - file_path_17: str, path to the CSV file for March 17th\n",
        "\n",
        "    Returns:\n",
        "    - data: DataFrame, combined dataset\n",
        "    \"\"\"\n",
        "    # Load datasets\n",
        "    data_16 = pd.read_csv(file_path_16)\n",
        "    data_17 = pd.read_csv(file_path_17)\n",
        "\n",
        "    # Combine datasets\n",
        "    data = pd.concat([data_16, data_17], ignore_index=True)\n",
        "    return data\n",
        "\n",
        "# 2 Data Preprocessing\n",
        "\n",
        "# data_preprocessing.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocess the cryptocurrency data.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame, the combined dataset\n",
        "\n",
        "    Returns:\n",
        "    - data: DataFrame, preprocessed dataset\n",
        "    \"\"\"\n",
        "    # Check for missing values\n",
        "    print(\"Missing values in each column:\")\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "    # Handle missing values (if any)\n",
        "    data.fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Normalize/scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    data[['price', '24h_volume', 'mkt_cap']] = scaler.fit_transform(data[['price', '24h_volume', 'mkt_cap']])\n",
        "\n",
        "    # Parse dates and sort\n",
        "    data['date'] = pd.to_datetime(data['date'])\n",
        "    data.sort_values('date', inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 3  Feature Engineering\n",
        "\n",
        "# feature_engineering.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def engineer_features(data):\n",
        "    \"\"\"\n",
        "    Perform feature engineering on the cryptocurrency data.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame, the preprocessed dataset\n",
        "\n",
        "    Returns:\n",
        "    - data: DataFrame, dataset with engineered features\n",
        "    \"\"\"\n",
        "    # Calculate Liquidity Ratio\n",
        "    data['liquidity_ratio'] = data['24h_volume'] / data['mkt_cap']\n",
        "\n",
        "    # Calculate Moving Averages\n",
        "    data['7d_moving_avg'] = data['price'].rolling(window=7).mean()\n",
        "    data['14d_moving_avg'] = data['price'].rolling(window=14).mean()\n",
        "\n",
        "    # Calculate Price Change Percentages\n",
        "    data['price_change_pct'] = data['price'].pct_change() * 100\n",
        "\n",
        "    # Calculate Volatility Indicators\n",
        "    data['volatility'] = data['price'].rolling(window=7).std()\n",
        "\n",
        "    return data\n",
        "\n",
        "# 4  Exploratory Data Analysis (EDA).\n",
        "\n",
        "# exploratory_analysis.py\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def perform_eda(data):\n",
        "    \"\"\"\n",
        "    Perform exploratory data analysis on the cryptocurrency data.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame, the dataset with engineered features\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Summary statistics\n",
        "    print(\"Summary Statistics:\")\n",
        "    print(data.describe())\n",
        "\n",
        "    # Correlation matrix\n",
        "    correlation_matrix = data.corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualizations\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(data['24h_volume'], data['price'])\n",
        "    plt.title('Price vs Volume')\n",
        "    plt.xlabel('24h Volume')\n",
        "    plt.ylabel('Price')\n",
        "    plt.show()\n",
        "\n",
        "    # Liquidity ratio trends\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(data['date'], data['liquidity_ratio'])\n",
        "    plt.title('Liquidity Ratio Trends')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Liquidity Ratio')\n",
        "    plt.show()\n",
        "\n",
        "# 5 Model Development\n",
        "\n",
        "# model_development.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def develop_model(data):\n",
        "    \"\"\"\n",
        "    Develop and evaluate a Random Forest model for predicting liquidity ratio.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame, the dataset with engineered features\n",
        "\n",
        "    Returns:\n",
        "    - metrics: dict, evaluation metrics of the model\n",
        "    \"\"\"\n",
        "    # Split data into features and target\n",
        "    X = data[['price', '24h_volume', 'mkt_cap', '7d_moving_avg', '14d_moving_avg']]\n",
        "    y = data['liquidity_ratio']\n",
        "\n",
        "    # Split into train/test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Use Random Forest Regressor\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'R²': r2\n",
        "    }\n",
        "\n",
        "    print(f'MAE: {mae}, RMSE: {rmse}, R²: {r2}')\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# 6  Hyperparameter Tuning\n",
        "\n",
        "# hyperparameter_tuning.py\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def tune_hyperparameters(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning for the Random Forest model using GridSearchCV.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: DataFrame, training features\n",
        "    - y_train: Series, training target\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dict, best hyperparameters found\n",
        "    \"\"\"\n",
        "    # Define the parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5]\n",
        "    }\n",
        "\n",
        "    # Initialize GridSearchCV\n",
        "    grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=3)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Print best parameters\n",
        "    print(f'Best parameters: {grid_search.best_params_}')\n",
        "    return grid_search.best_params_\n",
        "\n",
        "# 7 Model Evaluation\n",
        "\n",
        "# model_evaluation.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def evaluate_model(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate the model by plotting actual vs predicted values and printing evaluation scores.\n",
        "\n",
        "    Parameters:\n",
        "    - y_test: Series, actual target values\n",
        "    - y_pred: array, predicted target values\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Plot actual vs predicted liquidity\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(y_test, y_pred)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "    plt.title('Actual vs Predicted Liquidity Ratio')\n",
        "    plt.xlabel('Actual Liquidity Ratio')\n",
        "    plt.ylabel('Predicted Liquidity Ratio')\n",
        "    plt.show()\n",
        "\n",
        "    # Print evaluation scores\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f'MAE: {mae}')\n",
        "    print(f'RMSE: {rmse}')\n",
        "    print(f'R²: {r2}')\n",
        "\n",
        "#8 (Optional) Deployment Preview\n",
        "\n",
        "# model_evaluation.py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def evaluate_model(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate the model by plotting actual vs predicted values and printing evaluation scores.\n",
        "\n",
        "    Parameters:\n",
        "    - y_test: Series, actual target values\n",
        "    - y_pred: array, predicted target values\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Plot actual vs predicted liquidity\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(y_test, y_pred)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
        "    plt.title('Actual vs Predicted Liquidity Ratio')\n",
        "    plt.xlabel('Actual Liquidity Ratio')\n",
        "    plt.ylabel('Predicted Liquidity Ratio')\n",
        "    plt.show()\n",
        "\n",
        "    # Print evaluation scores\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f'MAE: {mae}')\n",
        "    print(f'RMSE: {rmse}')\n",
        "    print(f'R²: {r2}')\n",
        "\n"
      ],
      "metadata": {
        "id": "_3rBo9JCMZNi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction & Problem Statement\n",
        "Project Goal: The goal of this project is to analyze cryptocurrency market data from March 16th and 17th, 2022, to predict the liquidity ratio of various cryptocurrencies using machine learning techniques. The liquidity ratio is defined as the ratio of 24-hour trading volume to market capitalization.\n",
        "\n",
        "2.Solution Statement\n",
        "\n",
        "We collected and combined historical cryptocurrency data from two different dates.\n",
        "Feature engineering was applied to create liquidity indicators like the liquidity ratio.\n",
        "A machine learning model (Random Forest Regressor) was trained to predict liquidity.\n",
        "Model performance was evaluated using RMSE, MAE, and R² score.\n",
        "This solution helps identify low-liquidity scenarios early, supporting stable market decisions.\n",
        "\n",
        "Dataset used\n",
        "\n",
        "https://drive.google.com/file/d/1ihTpQ6Ej4HYL2upgPUmnu_g2mp1prG6N/view?usp=drive_link\n",
        "\n",
        "https://drive.google.com/file/d/1RKo7P1s35uqQDtkYjorzPNsVLW5SnPmU/view?usp=drive_link\n",
        "\n",
        "Tech Stack Used\n",
        "\n",
        "Python\n",
        "\n",
        "FastAPI\n",
        "\n",
        "Machine learning algorithms\n",
        "\n",
        "Docker\n",
        "\n",
        "MongoDB\n",
        "\n",
        "Infrastructure required\n",
        "\n",
        "AWS S3\n",
        "\n",
        "Azure\n",
        "\n",
        "Github Actions\n",
        "\n",
        "How to run\n",
        "\n",
        "2. Importing Libraries & Dataset\n",
        "\n",
        "Step 1: Create the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Write the Code\n",
        "\n",
        "# __init__.py\n",
        "\"\"\"\n",
        "Crypto Analysis Package\n",
        "\"\"\"\n",
        " requirements.txt\n",
        "\n",
        " pandas\n",
        "numpy\n",
        "matplotlib\n",
        "seaborn\n",
        "scikit-learn\n",
        "\n",
        "Step 3: Install the Package\n",
        "pip install -e .\n",
        "\n",
        "3. Data Preprocessing\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "4. Feature Engineering\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "└── feature_engineering.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "5. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "├── feature_engineering.py\n",
        "└── exploratory_analysis.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "6. Model Development\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "├── feature_engineering.py\n",
        "├── exploratory_analysis.py\n",
        "└── model_development.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "7. Hyperparameter Tuning\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "├── feature_engineering.py\n",
        "├── exploratory_analysis.py\n",
        "├── model_development.py\n",
        "└── hyperparameter_tuning.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "8. Model Evaluation\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "├── feature_engineering.py\n",
        "├── exploratory_analysis.py\n",
        "├── model_development.py\n",
        "├── hyperparameter_tuning.py\n",
        "└── model_evaluation.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "9. (Optional) Deployment Preview\n",
        "\n",
        "Step 1: Update the Package Structure\n",
        "\n",
        "crypto_analysis/\n",
        "│\n",
        "├── __init__.py\n",
        "├── data_loader.py\n",
        "├── data_preprocessing.py\n",
        "├── feature_engineering.py\n",
        "├── exploratory_analysis.py\n",
        "├── model_development.py\n",
        "├── hyperparameter_tuning.py\n",
        "└── model_evaluation.py\n",
        "└── requirements.txt\n",
        "\n",
        "Step 2: Install the Package\n",
        "pip install -e ."
      ],
      "metadata": {
        "id": "ZtzLFjj6VLTC"
      }
    }
  ]
}